{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cd896ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch    : 2.1.1+cpu\n",
      "torchtext: 0.5.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import tqdm\n",
    "import string\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.backends.cudnn.deterministic = True\n",
    "print(\"\\ntorch    :\", torch.__version__)\n",
    "print(\"torchtext:\", torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90eacb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "VOCABULARY_SIZE = 20000\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 150\n",
    "NUM_EPOCHS = 20\n",
    "DEVICE = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5f2dc6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_description</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ممتاز لكن مفروض زام مطاعم باضاف خدم الك نت</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>افاشل تطبيق تاخير وقت والاكل ناشف صراح اول مره...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>برنامج مهكر</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>يج لكنرع</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  review_description  rating\n",
       "0         ممتاز لكن مفروض زام مطاعم باضاف خدم الك نت       1\n",
       "1  افاشل تطبيق تاخير وقت والاكل ناشف صراح اول مره...      -1\n",
       "2                                                NaN      -1\n",
       "3                                        برنامج مهكر      -1\n",
       "4                                           يج لكنرع       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('C:/Users/Asus/Desktop/New folder/run 1/train2.1.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13240337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>review_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>اهن خدم عملاء محادث مباشر ما قصرو اله يوفق يعط...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ممتاز جدا لكن اتم ان تكون مسابق والجواز طلب سع...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>محمل يقول تم ايقاف حط عشان تسون خطاء</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>شغل طيب</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ماجرب</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                 review_description\n",
       "0   1  اهن خدم عملاء محادث مباشر ما قصرو اله يوفق يعط...\n",
       "1   2  ممتاز جدا لكن اتم ان تكون مسابق والجواز طلب سع...\n",
       "2   3               محمل يقول تم ايقاف حط عشان تسون خطاء\n",
       "3   4                                            شغل طيب\n",
       "4   5                                              ماجرب"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('C:/Users/Asus/Desktop/New folder/run 1/test2.01.csv')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99d5991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALEF_MADDA = u'\\u0622'\n",
    "ALEF_HAMZA_ABOVE = u'\\u0623'\n",
    "ALEF_HAMZA_BELOW = u'\\u0625'\n",
    "HAMZA_ABOVE = u'\\u0654'\n",
    "HAMZA_BELOW = u'\\u0655'\n",
    "ALEF = u'\\u0627'\n",
    "ALEFAT_PAT = re.compile(u\"[\"+u\"\".join([ALEF_MADDA, ALEF_HAMZA_ABOVE,\n",
    "                                       ALEF_HAMZA_BELOW, HAMZA_ABOVE,\n",
    "                                       HAMZA_BELOW])+u\"]\")\n",
    "\n",
    "ALEF_TANWEEN = u\"\\u0627\\u064B\"\n",
    "TASHKEEL = u\"[\\u064B-\\u0652]\"\n",
    "ALEFAT = u\"[\"+u\"\".join([ALEF_MADDA, ALEF_HAMZA_ABOVE,\n",
    "                                       ALEF_HAMZA_BELOW, HAMZA_ABOVE,\n",
    "                                       HAMZA_BELOW])+u\"]\"\n",
    "\n",
    "WAW_HAMZA = u'\\u0624'\n",
    "YEH_HAMZA = u'\\u0626'\n",
    "HAMZA = u'\\u0621'\n",
    "\n",
    "HAMZAT_PAT = re.compile(u\"[\"+u\"\".join([WAW_HAMZA, YEH_HAMZA])+u\"]\")\n",
    "\n",
    "TEH_MARBUTA = u'\\u0629'\n",
    "HEH = u'\\u0647'\n",
    "ALEF_MAKSURA = u'\\u0649'\n",
    "YEH = u'\\u064a'\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    from nltk import word_tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def remove_stop_words(tokens, mode=0):\n",
    "    if mode == 0:\n",
    "        from nltk.corpus import stopwords\n",
    "        stop_words = stopwords.words('arabic')\n",
    "        stop_words.append('ال')\n",
    "        filtered_text = [word for word in tokens if word not in stop_words]\n",
    "        return filtered_text\n",
    "    elif mode == 2:\n",
    "        from gensim.parsing.preprocessing import STOPWORDS\n",
    "        STOPWORDS = STOPWORDS.union(set(['ال']))\n",
    "        filtered_tokens = [token for token in tokens if token not in STOPWORDS]\n",
    "        filtered_text = ' '.join(filtered_tokens)\n",
    "        return filtered_text.split()\n",
    "    \n",
    "\n",
    "def snowball_stemmer(tokens):\n",
    "    from snowballstemmer import stemmer\n",
    "    arabic_stemmer = stemmer('arabic')\n",
    "    stemmed_words = arabic_stemmer.stemWords(tokens)\n",
    "    return stemmed_words\n",
    "\n",
    "\n",
    "def isris_stemmer(tokens):\n",
    "    from nltk.stem.isri import ISRIStemmer\n",
    "    stemmer = ISRIStemmer()\n",
    "    stemmed_list = [stemmer.stem(word) for word in tokens]\n",
    "    return stemmed_list\n",
    "\n",
    "\n",
    "def tashaphyne_stemmer(text):\n",
    "    from tashaphyne.stemming import ArabicLightStemmer\n",
    "    stem = ArabicLightStemmer()\n",
    "    return stem.light_stem(text)\n",
    "\n",
    "\n",
    "def farasa_stemmer(tokens):\n",
    "    from farasa.stemmer import FarasaStemmer\n",
    "    stemmer = FarasaStemmer()\n",
    "    stemmed_list = [stemmer.stem(word) for word in tokens]\n",
    "    return stemmed_list\n",
    "\n",
    "\n",
    "def farasa_segmenter(text):\n",
    "    from farasa.segmentation import FarasaSegmenter\n",
    "    segmenter = FarasaSegmenter()\n",
    "    segments = segmenter.segment(text)\n",
    "    # print(segments)\n",
    "    return re.split(r'\\s\\w{1,2}\\+|\\+\\w{1,2}\\s|\\s|\\+', segments)\n",
    "    # return re.split(r'\\+|\\s', segments)\n",
    "    \n",
    "\n",
    "def remove_non_arabic(text):\n",
    "    return ' '.join(re.sub(u\"[^\\u0621-\\u063A\\u0640-\\u0652 ]\", \" \", text,  flags=re.UNICODE).split())\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \n",
    "    cleaned_text = ''.join(char for char in text if char not in string.punctuation)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    #text = text.strip() #Remove end spaces\n",
    "    text = re.sub(u'%s' % ALEFAT, ALEF, text)\n",
    "    text = re.sub(u'%s' % ALEF_TANWEEN, \"\", text)\n",
    "    text = re.sub(u'%s' % TASHKEEL, \"\", text)\n",
    "    text = re.sub(u'[%s]' % ALEF_MAKSURA, YEH, text)\n",
    "    text = re.sub(u'[%s]' % TEH_MARBUTA, HEH, text)\n",
    "    text = re.sub(u'[%s]' % WAW_HAMZA, HAMZA, text)\n",
    "    text = re.sub(u'[%s]' % YEH_HAMZA, HAMZA, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess(text, token_mode=0, stem_mode=0):\n",
    "    # print(\"Original Text:\", text)\n",
    "    \n",
    "    #removed_non_arabic = remove_non_arabic(text)\n",
    "    \n",
    "    \n",
    "    #normalized_text = normalize_text(removed_non_arabic)\n",
    "    remove_arabic=remove_punctuation(text)\n",
    "    \n",
    "    if token_mode == 0:        \n",
    "        stemmed_tokens = farasa_segmenter(remove_arabic)\n",
    "    \n",
    "    elif token_mode == 1:\n",
    "        tokens = tokenize_text(remove_arabic)\n",
    "        if stem_mode == 0:\n",
    "            stemmed_tokens = snowball_stemmer(tokens)\n",
    "            # print(\"Snowball Stemmed:\", stemmed_tokens)\n",
    "        \n",
    "        elif stem_mode == 1:\n",
    "            stemmed_tokens = isris_stemmer(tokens)\n",
    "        \n",
    "        elif stem_mode == 2:\n",
    "            stemmed_tokens = farasa_stemmer(tokens)\n",
    "\n",
    "    \n",
    "    removed_stop_words = remove_stop_words(stemmed_tokens, mode=0)\n",
    "    return removed_stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e85f6b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'] = df['review_description']\n",
    "column_to_move = 'processed_text'\n",
    "first_column = df.pop(column_to_move)\n",
    "df.insert(0, column_to_move, first_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33c1b40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of nulls in train_texts: 17\n",
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>اهن خدم عملاء محادث مباشر ما قصرو اله يوفق يعط...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ممتاز جدا لكن اتم ان تكون مسابق والجواز طلب سع...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>محمل يقول تم ايقاف حط عشان تسون خطاء</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>شغل طيب</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ماجرب</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      processed_text\n",
       "0  اهن خدم عملاء محادث مباشر ما قصرو اله يوفق يعط...\n",
       "1  ممتاز جدا لكن اتم ان تكون مسابق والجواز طلب سع...\n",
       "2               محمل يقول تم ايقاف حط عشان تسون خطاء\n",
       "3                                            شغل طيب\n",
       "4                                              ماجرب"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "null_indices_train_texts = df2[df2['review_description'].isnull()].index\n",
    "print(\"Indices of nulls in train_texts:\", len(null_indices_train_texts))\n",
    "df2['review_description'].fillna(\"\", inplace=True)\n",
    "print(len(df2))\n",
    "\n",
    "# Remove rows with null values in 'review_description' column\n",
    "df = df.dropna(subset=['review_description'])\n",
    "\n",
    "df.drop('review_description', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('C:/Users/Asus/Desktop/New folder (4)/processed_data.csv', index=False)\n",
    "\n",
    "df.head()\n",
    "\n",
    "df2['processed_text'] = df2['review_description']\n",
    "df2.drop('review_description', axis=1, inplace=True)\n",
    "df2.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df2.to_csv('C:/Users/Asus/Desktop/New folder (4)/processed_data2.csv', index=False)\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9f488d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb3cfccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.dataset.TabularDataset at 0x1512b91dd90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "TEXT = torchtext.data.Field(\n",
    "    tokenize=lambda text: text.split(),  \n",
    "    include_lengths=True\n",
    ")\n",
    "LABEL = torchtext.data.LabelField()\n",
    "\n",
    "\n",
    "fields = [('processed_text', TEXT), ('rating', LABEL)]\n",
    "\n",
    "\n",
    "dataset = torchtext.data.TabularDataset(\n",
    "    path='C:/Users/Asus/Desktop/New folder (4)/processed_data.csv', format='csv',\n",
    "    skip_header=True, fields=fields\n",
    ")\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f486417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'torchtext.data' from 'C:\\\\Users\\\\Asus\\\\anaconda3\\\\lib\\\\site-packages\\\\torchtext\\\\data\\\\__init__.py'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torchtext.data.dataset.TabularDataset at 0x1512b91d4c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fields2 = [('processed_text', TEXT)]\n",
    "\n",
    "\n",
    "dataset2 = torchtext.data.TabularDataset(\n",
    "    path='C:/Users/Asus/Desktop/New folder (4)/processed_data2.csv', format='csv',\n",
    "    skip_header=True, fields=fields2\n",
    ")\n",
    "\n",
    "print(torchtext.data)\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "938949a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 36834\n",
      "Num Test: 1000\n"
     ]
    }
   ],
   "source": [
    "#Split Dataset into Train/Validation/Test\n",
    "\n",
    "train_data= dataset\n",
    "test_data = dataset2\n",
    "\n",
    "print(f'Num Train: {len(train_data)}')\n",
    "print(f'Num Test: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46617087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 31309\n",
      "Num Validation: 5525\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = train_data.split(\n",
    "    split_ratio=[0.85, 0.15],\n",
    "    random_state=random.seed(RANDOM_SEED))\n",
    "\n",
    "print(f'Num Train: {len(train_data)}')\n",
    "print(f'Num Validation: {len(valid_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94511fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'processed_text': ['ممتاز', 'ممتاز', 'ممتاز'], 'rating': '0'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92116368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'processed_text': ['اهن', 'خدم', 'عملاء', 'محادث', 'مباشر', 'ما', 'قصرو', 'اله', 'يوفق', 'يعط', 'الف', 'عاف']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(test_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce548f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 17757\n",
      "Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "#Build Vocabulary\n",
    "TEXT.build_vocab(train_data, max_size=VOCABULARY_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(f'Vocabulary size: {len(TEXT.vocab)}')\n",
    "print(f'Number of classes: {len(LABEL.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3784fefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ممتاز', 12604), ('تطبيق', 5665), ('جدا', 5063), ('طلب', 4345), ('لكن', 3944), ('فاشل', 3670), ('توصيل', 2460), ('ما', 2437), ('لا', 2319), ('مطاعم', 2248), ('انا', 1863), ('برنامج', 1790), ('خدم', 1763), ('رنامج', 1761), ('جميل', 1747), ('مش', 1718), ('ان', 1515), ('مطعم', 1386), ('مره', 1328), ('طلبا', 1264)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bc1514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'ممتاز', 'تطبيق', 'جدا', 'طلب', 'لكن', 'فاشل', 'توصيل', 'ما']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd14000d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'1': 0, '-1': 1, '0': 2})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2fd81b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Data Loaders\n",
    "train_loader, valid_loader, test_loader = \\\n",
    "    torchtext.data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data), \n",
    "        batch_size=BATCH_SIZE,\n",
    "        sort_within_batch=True, \n",
    "             sort_key=lambda x: len(x.processed_text),\n",
    "        shuffle=False,\n",
    "        device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0861760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 1\n",
    "# for batch in train_loader:\n",
    "#     # Get the padded sequences from the batch\n",
    "#     padded_sequences = batch.processed_text\n",
    "    \n",
    "#     # Extract the tensor from the tuple (assuming it's at index 0)\n",
    "#     padded_sequences = padded_sequences[0]\n",
    "    \n",
    "#     # Transpose the tensor to iterate through sentences\n",
    "#     padded_sequences = padded_sequences.transpose(0, 1)  # Assuming sentences are along dimension 1\n",
    "    \n",
    "#     # Convert numeric tokens to words using the vocabulary from TEXT\n",
    "#     word_sequences = [\n",
    "#         [TEXT.vocab.itos[token] for token in sentence]\n",
    "#         for sentence in padded_sequences\n",
    "#     ]\n",
    "    \n",
    "#     # Get the actual lengths of each sequence in the batch\n",
    "#     sequence_lengths = [len(seq) for seq in word_sequences]\n",
    "    \n",
    "#     # Get targets from the batch (assuming they are named 'targets')\n",
    "#     targets = batch.rating  # Replace 'targets' with the actual name of your target attribute\n",
    "#     #print(targets)\n",
    "#     # Print the lengths of sentences in the current batch\n",
    "#     #print(\"Lengths of sentences in the current batch:\", sequence_lengths)\n",
    "    \n",
    "#     # Print all sentences and their targets in the current batch\n",
    "#     print(\"Sentences in Batch {}:\".format(i))\n",
    "#     for idx, (sentence, target) in enumerate(zip(word_sequences, targets)):\n",
    "#         print(f\"Sentence {idx + 1}:\", sentence)\n",
    "#         print(f\"Target {idx + 1}:\", target)\n",
    "    \n",
    "#     i += 1\n",
    "#     print(\"Batch\", i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b11b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 1\n",
    "# for batch in train_loader:\n",
    "#     # Get the padded sequences from the batch\n",
    "#     padded_sequences = batch.processed_text\n",
    "   \n",
    "#     # Extract the tensor from the tuple (assuming it's at index 0)\n",
    "#     padded_sequences = padded_sequences[0]\n",
    "#     # Transpose the tensor to iterate through sentences\n",
    "#     padded_sequences = padded_sequences.transpose(0, 1)  # Assuming sentences are along dimension 1\n",
    "#     # Get the actual lengths of each sequence in the batch\n",
    "#     sequence_lengths = [len(seq) for seq in padded_sequences]\n",
    "#     print(padded_sequences)\n",
    "#     print(sequence_lengths[0])\n",
    "    \n",
    "#     i += 1\n",
    "#     print(\"Batch\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3a778ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 1\n",
    "# for batch in test_loader:\n",
    "#     # Get the padded sequences from the batch\n",
    "#     padded_sequences = batch.processed_text\n",
    "    \n",
    "#     # Extract the tensor from the tuple (assuming it's at index 0)\n",
    "#     padded_sequences = padded_sequences[0]\n",
    "#     # Transpose the tensor to iterate through sentences\n",
    "#     padded_sequences = padded_sequences.transpose(0, 1)  # Assuming sentences are along dimension 1\n",
    "#     # Get the actual lengths of each sequence in the batch\n",
    "#     sequence_lengths = [len(seq) for seq in padded_sequences]\n",
    "#     print(padded_sequences)\n",
    "#     print(sequence_lengths[0])\n",
    "    \n",
    "#     i += 1\n",
    "#     print(\"Batch\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "303ebc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Text matrix size: torch.Size([1, 150])\n",
      "Target vector size: torch.Size([150])\n",
      "\n",
      "Valid:\n",
      "Text matrix size: torch.Size([1, 150])\n",
      "Target vector size: torch.Size([150])\n",
      "\n",
      "Test:\n",
      "Text matrix size: torch.Size([2, 150])\n"
     ]
    }
   ],
   "source": [
    "#esting the iterators (note that the number of rows depends on the longest document in the respective batch):\n",
    "print('Train')\n",
    "for batch in train_loader:\n",
    "    print(f'Text matrix size: {batch.processed_text[0].size()}')\n",
    "    print(f'Target vector size: {batch.rating.size()}')\n",
    "    break\n",
    "    \n",
    "print('\\nValid:')\n",
    "for batch in valid_loader:\n",
    "    print(f'Text matrix size: {batch.processed_text[0].size()}')\n",
    "    print(f'Target vector size: {batch.rating.size()}')\n",
    "    break\n",
    "    \n",
    "print('\\nTest:')\n",
    "for batch in test_loader:\n",
    "    print(f'Text matrix size: {batch.processed_text[0].size()}')\n",
    "   \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df8c254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = torch.nn.LSTM(embedding_dim, hidden_dim)        \n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, text, text_length):\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, text_length, batch_first=True)\n",
    "        packed_output, (hidden, cell) = self.rnn(packed)\n",
    "        \n",
    "        # Unpack the sequence (assuming batch_first=True)\n",
    "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Take the last time-step's output\n",
    "        last_output = output[:, -1, :]\n",
    "        \n",
    "        output = self.fc(last_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e014c35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (embedding): Embedding(17757, 128)\n",
       "  (rnn): LSTM(128, 256)\n",
       "  (fc): Linear(in_features=256, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model, criterion, and optimizer\n",
    "model = LSTMModel(len(TEXT.vocab), EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Move model to device\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "535b152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct_pred, num_examples = 0, 0\n",
    "\n",
    "        for batch_idx, batch_data in enumerate(data_loader):\n",
    "\n",
    "            # Get the padded sequences from the batch\n",
    "            padded_sequences3 = batch_data.processed_text\n",
    "\n",
    "            # Extract the tensor from the tuple (assuming it's at index 0)\n",
    "            padded_sequences3 = padded_sequences3[0]\n",
    "\n",
    "            # Transpose the tensor to iterate through sentences\n",
    "            padded_sequences3 = padded_sequences3.transpose(0, 1)  # Assuming sentences are along dimension 1\n",
    "\n",
    "\n",
    "            sequence_lengths3 = [len(seq) for seq in padded_sequences3]\n",
    "\n",
    "            # Get targets from the batch (assuming they are named 'targets')\n",
    "            targets3 = batch_data.rating  # Replace 'targets' with the actual name of your target attribute\n",
    "            \n",
    "            logits = model(padded_sequences3, sequence_lengths3)\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "            num_examples += targets3.size(0)\n",
    "\n",
    "            correct_pred += (predicted_labels == targets3).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3aaa3cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/020 | Batch 000/209 | Loss: 1.0980\n",
      "Epoch: 001/020 | Batch 050/209 | Loss: 0.9503\n",
      "Epoch: 001/020 | Batch 100/209 | Loss: 0.9671\n",
      "Epoch: 001/020 | Batch 150/209 | Loss: 0.9521\n",
      "Epoch: 001/020 | Batch 200/209 | Loss: 1.0150\n",
      "training accuracy: 64.39%\n",
      "valid accuracy: 64.27%\n",
      "Time elapsed: 0.68 min\n",
      "Epoch: 002/020 | Batch 000/209 | Loss: 0.9314\n",
      "Epoch: 002/020 | Batch 050/209 | Loss: 0.7552\n",
      "Epoch: 002/020 | Batch 100/209 | Loss: 0.9224\n",
      "Epoch: 002/020 | Batch 150/209 | Loss: 0.8637\n",
      "Epoch: 002/020 | Batch 200/209 | Loss: 0.9083\n",
      "training accuracy: 67.56%\n",
      "valid accuracy: 67.71%\n",
      "Time elapsed: 1.33 min\n",
      "Epoch: 003/020 | Batch 000/209 | Loss: 0.9043\n",
      "Epoch: 003/020 | Batch 050/209 | Loss: 0.7181\n",
      "Epoch: 003/020 | Batch 100/209 | Loss: 0.8960\n",
      "Epoch: 003/020 | Batch 150/209 | Loss: 0.8350\n",
      "Epoch: 003/020 | Batch 200/209 | Loss: 0.8575\n",
      "training accuracy: 69.12%\n",
      "valid accuracy: 68.71%\n",
      "Time elapsed: 2.09 min\n",
      "Epoch: 004/020 | Batch 000/209 | Loss: 0.8805\n",
      "Epoch: 004/020 | Batch 050/209 | Loss: 0.6851\n",
      "Epoch: 004/020 | Batch 100/209 | Loss: 0.8770\n",
      "Epoch: 004/020 | Batch 150/209 | Loss: 0.8108\n",
      "Epoch: 004/020 | Batch 200/209 | Loss: 0.8145\n",
      "training accuracy: 70.63%\n",
      "valid accuracy: 70.24%\n",
      "Time elapsed: 2.74 min\n",
      "Epoch: 005/020 | Batch 000/209 | Loss: 0.8567\n",
      "Epoch: 005/020 | Batch 050/209 | Loss: 0.6637\n",
      "Epoch: 005/020 | Batch 100/209 | Loss: 0.8568\n",
      "Epoch: 005/020 | Batch 150/209 | Loss: 0.7953\n",
      "Epoch: 005/020 | Batch 200/209 | Loss: 0.7773\n",
      "training accuracy: 72.21%\n",
      "valid accuracy: 71.13%\n",
      "Time elapsed: 3.41 min\n",
      "Epoch: 006/020 | Batch 000/209 | Loss: 0.8304\n",
      "Epoch: 006/020 | Batch 050/209 | Loss: 0.6444\n",
      "Epoch: 006/020 | Batch 100/209 | Loss: 0.8322\n",
      "Epoch: 006/020 | Batch 150/209 | Loss: 0.7845\n",
      "Epoch: 006/020 | Batch 200/209 | Loss: 0.7407\n",
      "training accuracy: 72.71%\n",
      "valid accuracy: 71.78%\n",
      "Time elapsed: 4.07 min\n",
      "Epoch: 007/020 | Batch 000/209 | Loss: 0.8047\n",
      "Epoch: 007/020 | Batch 050/209 | Loss: 0.6348\n",
      "Epoch: 007/020 | Batch 100/209 | Loss: 0.8109\n",
      "Epoch: 007/020 | Batch 150/209 | Loss: 0.7665\n",
      "Epoch: 007/020 | Batch 200/209 | Loss: 0.7562\n",
      "training accuracy: 72.74%\n",
      "valid accuracy: 71.48%\n",
      "Time elapsed: 4.76 min\n",
      "Epoch: 008/020 | Batch 000/209 | Loss: 0.7837\n",
      "Epoch: 008/020 | Batch 050/209 | Loss: 0.6280\n",
      "Epoch: 008/020 | Batch 100/209 | Loss: 0.8007\n",
      "Epoch: 008/020 | Batch 150/209 | Loss: 0.7662\n",
      "Epoch: 008/020 | Batch 200/209 | Loss: 0.7267\n",
      "training accuracy: 74.98%\n",
      "valid accuracy: 72.89%\n",
      "Time elapsed: 5.43 min\n",
      "Epoch: 009/020 | Batch 000/209 | Loss: 0.7727\n",
      "Epoch: 009/020 | Batch 050/209 | Loss: 0.6060\n",
      "Epoch: 009/020 | Batch 100/209 | Loss: 0.7853\n",
      "Epoch: 009/020 | Batch 150/209 | Loss: 0.7483\n",
      "Epoch: 009/020 | Batch 200/209 | Loss: 0.7081\n",
      "training accuracy: 75.62%\n",
      "valid accuracy: 73.16%\n",
      "Time elapsed: 6.12 min\n",
      "Epoch: 010/020 | Batch 000/209 | Loss: 0.7571\n",
      "Epoch: 010/020 | Batch 050/209 | Loss: 0.5953\n",
      "Epoch: 010/020 | Batch 100/209 | Loss: 0.7698\n",
      "Epoch: 010/020 | Batch 150/209 | Loss: 0.7320\n",
      "Epoch: 010/020 | Batch 200/209 | Loss: 0.6920\n",
      "training accuracy: 76.31%\n",
      "valid accuracy: 73.54%\n",
      "Time elapsed: 6.78 min\n",
      "Epoch: 011/020 | Batch 000/209 | Loss: 0.7417\n",
      "Epoch: 011/020 | Batch 050/209 | Loss: 0.5832\n",
      "Epoch: 011/020 | Batch 100/209 | Loss: 0.7563\n",
      "Epoch: 011/020 | Batch 150/209 | Loss: 0.7146\n",
      "Epoch: 011/020 | Batch 200/209 | Loss: 0.6780\n",
      "training accuracy: 77.03%\n",
      "valid accuracy: 74.01%\n",
      "Time elapsed: 7.43 min\n",
      "Epoch: 012/020 | Batch 000/209 | Loss: 0.7267\n",
      "Epoch: 012/020 | Batch 050/209 | Loss: 0.5717\n",
      "Epoch: 012/020 | Batch 100/209 | Loss: 0.7435\n",
      "Epoch: 012/020 | Batch 150/209 | Loss: 0.6998\n",
      "Epoch: 012/020 | Batch 200/209 | Loss: 0.6710\n",
      "training accuracy: 77.24%\n",
      "valid accuracy: 73.88%\n",
      "Time elapsed: 8.13 min\n",
      "Epoch: 013/020 | Batch 000/209 | Loss: 0.7150\n",
      "Epoch: 013/020 | Batch 050/209 | Loss: 0.5607\n",
      "Epoch: 013/020 | Batch 100/209 | Loss: 0.7341\n",
      "Epoch: 013/020 | Batch 150/209 | Loss: 0.6844\n",
      "Epoch: 013/020 | Batch 200/209 | Loss: 0.6639\n",
      "training accuracy: 78.82%\n",
      "valid accuracy: 75.26%\n",
      "Time elapsed: 8.84 min\n",
      "Epoch: 014/020 | Batch 000/209 | Loss: 0.7037\n",
      "Epoch: 014/020 | Batch 050/209 | Loss: 0.5433\n",
      "Epoch: 014/020 | Batch 100/209 | Loss: 0.7226\n",
      "Epoch: 014/020 | Batch 150/209 | Loss: 0.6597\n",
      "Epoch: 014/020 | Batch 200/209 | Loss: 0.6498\n",
      "training accuracy: 78.95%\n",
      "valid accuracy: 75.35%\n",
      "Time elapsed: 9.70 min\n",
      "Epoch: 015/020 | Batch 000/209 | Loss: 0.6903\n",
      "Epoch: 015/020 | Batch 050/209 | Loss: 0.5356\n",
      "Epoch: 015/020 | Batch 100/209 | Loss: 0.7103\n",
      "Epoch: 015/020 | Batch 150/209 | Loss: 0.6446\n",
      "Epoch: 015/020 | Batch 200/209 | Loss: 0.6334\n",
      "training accuracy: 79.60%\n",
      "valid accuracy: 75.82%\n",
      "Time elapsed: 10.40 min\n",
      "Epoch: 016/020 | Batch 000/209 | Loss: 0.6762\n",
      "Epoch: 016/020 | Batch 050/209 | Loss: 0.5251\n",
      "Epoch: 016/020 | Batch 100/209 | Loss: 0.6964\n",
      "Epoch: 016/020 | Batch 150/209 | Loss: 0.6499\n",
      "Epoch: 016/020 | Batch 200/209 | Loss: 0.6406\n",
      "training accuracy: 80.42%\n",
      "valid accuracy: 75.91%\n",
      "Time elapsed: 11.05 min\n",
      "Epoch: 017/020 | Batch 000/209 | Loss: 0.6703\n",
      "Epoch: 017/020 | Batch 050/209 | Loss: 0.5095\n",
      "Epoch: 017/020 | Batch 100/209 | Loss: 0.6904\n",
      "Epoch: 017/020 | Batch 150/209 | Loss: 0.6214\n",
      "Epoch: 017/020 | Batch 200/209 | Loss: 0.6277\n",
      "training accuracy: 80.54%\n",
      "valid accuracy: 76.49%\n",
      "Time elapsed: 11.71 min\n",
      "Epoch: 018/020 | Batch 000/209 | Loss: 0.6579\n",
      "Epoch: 018/020 | Batch 050/209 | Loss: 0.4812\n",
      "Epoch: 018/020 | Batch 100/209 | Loss: 0.6778\n",
      "Epoch: 018/020 | Batch 150/209 | Loss: 0.6079\n",
      "Epoch: 018/020 | Batch 200/209 | Loss: 0.6202\n",
      "training accuracy: 82.60%\n",
      "valid accuracy: 77.81%\n",
      "Time elapsed: 12.41 min\n",
      "Epoch: 019/020 | Batch 000/209 | Loss: 0.6467\n",
      "Epoch: 019/020 | Batch 050/209 | Loss: 0.4733\n",
      "Epoch: 019/020 | Batch 100/209 | Loss: 0.6708\n",
      "Epoch: 019/020 | Batch 150/209 | Loss: 0.5880\n",
      "Epoch: 019/020 | Batch 200/209 | Loss: 0.6113\n",
      "training accuracy: 81.77%\n",
      "valid accuracy: 77.52%\n",
      "Time elapsed: 13.09 min\n",
      "Epoch: 020/020 | Batch 000/209 | Loss: 0.6377\n",
      "Epoch: 020/020 | Batch 050/209 | Loss: 0.4570\n",
      "Epoch: 020/020 | Batch 100/209 | Loss: 0.6556\n",
      "Epoch: 020/020 | Batch 150/209 | Loss: 0.5952\n",
      "Epoch: 020/020 | Batch 200/209 | Loss: 0.6091\n",
      "training accuracy: 80.53%\n",
      "valid accuracy: 76.11%\n",
      "Time elapsed: 13.73 min\n",
      "Total Training Time: 13.73 min\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    i=1\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        # Get the padded sequences from the batch\n",
    "        padded_sequences2 = batch_data.processed_text\n",
    "\n",
    "        # Extract the tensor from the tuple (assuming it's at index 0)\n",
    "        padded_sequences2 = padded_sequences2[0]\n",
    "\n",
    "        # Transpose the tensor to iterate through sentences\n",
    "        padded_sequences2 = padded_sequences2.transpose(0, 1)  # Assuming sentences are along dimension 1\n",
    "\n",
    "\n",
    "        sequence_lengths2 = [len(seq) for seq in padded_sequences2]\n",
    "\n",
    "        # Get targets from the batch (assuming they are named 'targets')\n",
    "        targets2 = batch_data.rating  # Replace 'targets' with the actual name of your target attribute\n",
    "        #print(targets2)\n",
    "        # Print the lengths of sentences in the current batch\n",
    "        #print(\"Lengths of sentences in the current batch:\", sequence_lengths2)\n",
    "\n",
    "        i += 1\n",
    "        #print(\"Batch\", i)\n",
    "\n",
    "\n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(padded_sequences2,sequence_lengths2)\n",
    "        #print(logits)\n",
    "        loss = F.cross_entropy(logits, targets2)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "#print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8d20f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy2(model, data_loader, device):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct_pred, num_examples = 0, 0\n",
    "        predicted=[]\n",
    "        for batch_idx, batch_data in enumerate(data_loader):\n",
    "            # Get the padded sequences from the batch\n",
    "            padded_sequences3 = batch_data.processed_text\n",
    "\n",
    "            # Extract the tensor from the tuple (assuming it's at index 0)\n",
    "            padded_sequences3 = padded_sequences3[0]\n",
    "\n",
    "            # Transpose the tensor to iterate through sentences\n",
    "            padded_sequences3 = padded_sequences3.transpose(0, 1)  # Assuming sentences are along dimension 1\n",
    "\n",
    "\n",
    "            sequence_lengths3 = [len(seq) for seq in padded_sequences3]\n",
    "\n",
    "            \n",
    "\n",
    "            logits = model(padded_sequences3, sequence_lengths3)\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "            predicted+=predicted_labels.tolist()\n",
    "            \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09d3885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_out=compute_accuracy2(model, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40f3a885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_values(lst):\n",
    "    for i in range(len(lst)):\n",
    "        if lst[i] == 0:\n",
    "            lst[i] = 1\n",
    "        elif lst[i] == 1:\n",
    "            lst[i] = -1\n",
    "        elif lst[i] == 2:\n",
    "            lst[i] = 0\n",
    "    return lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8f15636",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_out2=replace_values(predicted_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46ef4dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'C:/Users/Asus/Desktop/New folder (4)/predicted_ratings4.csv' has been created.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "# File name for the CSV file\n",
    "file_name = 'C:/Users/Asus/Desktop/New folder (4)/predicted_ratings4.csv'\n",
    "\n",
    "# Writing data to a CSV file\n",
    "with open(file_name, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write header\n",
    "    writer.writerow(['ID', 'rating'])\n",
    "\n",
    "    # Write data rows\n",
    "    for idx, rating in enumerate(predicted_out2, start=1):\n",
    "        writer.writerow([idx, rating])\n",
    "\n",
    "print(f\"CSV file '{file_name}' has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ee8a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
